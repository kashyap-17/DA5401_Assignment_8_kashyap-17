{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7740dc34-61e9-481e-a010-4ae729cac03c",
   "metadata": {},
   "source": [
    "# Part A Data Preprocessing and Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4b7bf-4b70-43ff-baad-041a9ec57095",
   "metadata": {},
   "source": [
    "## A.1 Data Loading and Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2564df0c-d422-4ad5-bb2c-738d3fe3c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set a random state for reproducibility\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aeab32-5758-43cb-884f-e552872e255b",
   "metadata": {},
   "source": [
    "We'll load the hour.csv file and take a first look at its structure, data types, and any potential missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34eb8630-0242-4c10-96e7-0046dd57655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Head ---\n",
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n",
      "\n",
      "--- Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17379 entries, 0 to 17378\n",
      "Data columns (total 17 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     17379 non-null  int64  \n",
      " 1   dteday      17379 non-null  object \n",
      " 2   season      17379 non-null  int64  \n",
      " 3   yr          17379 non-null  int64  \n",
      " 4   mnth        17379 non-null  int64  \n",
      " 5   hr          17379 non-null  int64  \n",
      " 6   holiday     17379 non-null  int64  \n",
      " 7   weekday     17379 non-null  int64  \n",
      " 8   workingday  17379 non-null  int64  \n",
      " 9   weathersit  17379 non-null  int64  \n",
      " 10  temp        17379 non-null  float64\n",
      " 11  atemp       17379 non-null  float64\n",
      " 12  hum         17379 non-null  float64\n",
      " 13  windspeed   17379 non-null  float64\n",
      " 14  casual      17379 non-null  int64  \n",
      " 15  registered  17379 non-null  int64  \n",
      " 16  cnt         17379 non-null  int64  \n",
      "dtypes: float64(4), int64(12), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df_hour = pd.read_csv('hour.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'hour.csv' not found. Please download it from the UCI repository.\")\n",
    "\n",
    "print(\"--- Data Head ---\")\n",
    "print(df_hour.head())\n",
    "print(\"\\n--- Data Info ---\")\n",
    "df_hour.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92b8a2-ef54-4413-8cb3-23b6a35ef9f8",
   "metadata": {},
   "source": [
    "The data is loaded successfully. We have 17,379 hourly entries.\n",
    "\n",
    "There are no missing values, as indicated by the info() output.\n",
    "\n",
    "dteday is an object (string), which we'll be dropping.\n",
    "\n",
    "Most features are int64 or float64, which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f725bbb-d6c3-4b12-a534-aa991bb3ffa7",
   "metadata": {},
   "source": [
    "### Dropping Irrelevant & Leakage Columns\n",
    "As per the assignment, we will drop instant, dteday, casual, and registered.\n",
    "\n",
    "instant: This is just a row index and provides no predictive information.\n",
    "\n",
    "dteday: This date column is redundant. All its information (year, month, day) is already captured in yr, mnth, weekday, and hr.\n",
    "\n",
    "casual and registered: This is the most critical drop. These two columns sum up to our target variable, cnt. Including them would be data leakage, leading to a perfect but useless model (it's like predicting a total score when you already know the component scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0b3cf9e-b807-4c6c-940c-8a080538554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original columns: 17\n",
      "Processed columns: 13\n",
      "Dropped 'instant', 'dteday', 'casual', 'registered'.\n"
     ]
    }
   ],
   "source": [
    "# Drop the specified columns\n",
    "df_processed = df_hour.drop(['instant', 'dteday', 'casual', 'registered'], axis=1)\n",
    "\n",
    "print(f\"\\nOriginal columns: {df_hour.shape[1]}\")\n",
    "print(f\"Processed columns: {df_processed.shape[1]}\")\n",
    "print(\"Dropped 'instant', 'dteday', 'casual', 'registered'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bb3ab-9b08-40ff-9ccd-d4e6aae13c06",
   "metadata": {},
   "source": [
    "### Feature Engineering: A Hybrid Approach\n",
    "\n",
    "Next, we convert our categorical features into a numerical format. This is the most critical step for model performance, and we must be precise. Our features fall into two distinct groups:\n",
    "\n",
    "1.  **Cyclical Features:** `hr`, `mnth`, and `weekday`.\n",
    "2.  **Nominal Features:** `season` and `weathersit`.\n",
    "\n",
    "A common mistake is to use One-Hot Encoding (OHE) for all of them. However, that would make our model *worse*. For example, OHE treats \"hour 0\" (midnight) and \"hour 23\" (11 PM) as completely unrelated, when in reality, they are adjacent.\n",
    "\n",
    "**Our Expert Strategy:**\n",
    "\n",
    "* **Cyclical Encoding (`sin`/`cos`):** For `hr`, `mnth`, and `weekday`, we will use a mathematical transformation. By converting them into `sin` and `cos` components, we map them onto a circle. This explicitly tells the model that \"hour 23\" is very close to \"hour 0,\" and \"month 12\" is very close to \"month 1.\" This should provide a massive boost in predictive power.\n",
    "* **One-Hot Encoding (`pd.get_dummies`):** For `season` and `weathersit`, which are nominal (unordered categories), OHE remains the correct approach. We'll set `drop_first=True` to prevent multicollinearity (the \"dummy variable trap\"), which is important for our Linear Regression baseline.\n",
    "\n",
    "This hybrid approach gives our models the best possible representation of the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eaca3ca-762f-4257-8b2f-3039bfbd35a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Head After *Cyclical* Preprocessing ---\n",
      "   yr  holiday  workingday  temp   atemp   hum  windspeed  cnt    hr_sin  \\\n",
      "0   0        0           0  0.24  0.2879  0.81        0.0   16  0.000000   \n",
      "1   0        0           0  0.22  0.2727  0.80        0.0   40  0.258819   \n",
      "2   0        0           0  0.22  0.2727  0.80        0.0   32  0.500000   \n",
      "3   0        0           0  0.24  0.2879  0.75        0.0   13  0.707107   \n",
      "4   0        0           0  0.24  0.2879  0.75        0.0    1  0.866025   \n",
      "\n",
      "     hr_cos  mnth_sin  mnth_cos  weekday_sin  weekday_cos  season_2  season_3  \\\n",
      "0  1.000000       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "1  0.965926       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "2  0.866025       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "3  0.707107       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "4  0.500000       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "\n",
      "   season_4  weathersit_2  weathersit_3  weathersit_4  \n",
      "0     False         False         False         False  \n",
      "1     False         False         False         False  \n",
      "2     False         False         False         False  \n",
      "3     False         False         False         False  \n",
      "4     False         False         False         False  \n",
      "\n",
      "Total features after new preprocessing: 20\n"
     ]
    }
   ],
   "source": [
    "# --- Cyclical Feature Engineering ---\n",
    "# Apply sin/cos transformations to cyclical features\n",
    "\n",
    "# Handle 'hr' (0-23, so a 24-hour cycle)\n",
    "df_processed['hr_sin'] = np.sin(2 * np.pi * df_processed['hr'] / 24.0)\n",
    "df_processed['hr_cos'] = np.cos(2 * np.pi * df_processed['hr'] / 24.0)\n",
    "\n",
    "# Handle 'mnth' (1-12, so a 12-month cycle)\n",
    "df_processed['mnth_sin'] = np.sin(2 * np.pi * df_processed['mnth'] / 12.0)\n",
    "df_processed['mnth_cos'] = np.cos(2 * np.pi * df_processed['mnth'] / 12.0)\n",
    "\n",
    "# Handle 'weekday' (0-6, so a 7-day cycle)\n",
    "df_processed['weekday_sin'] = np.sin(2 * np.pi * df_processed['weekday'] / 7.0)\n",
    "df_processed['weekday_cos'] = np.cos(2 * np.pi * df_processed['weekday'] / 7.0)\n",
    "\n",
    "# --- One-Hot Encoding for *non-cyclical* categories ---\n",
    "# 'season' and 'weathersit' are nominal, so OHE is still correct here.\n",
    "non_cyclical_cats = ['season', 'weathersit']\n",
    "df_processed = pd.get_dummies(df_processed, \n",
    "                              columns=non_cyclical_cats, \n",
    "                              drop_first=True)\n",
    "\n",
    "# Now we can drop the original categorical columns\n",
    "df_processed = df_processed.drop(['hr', 'mnth', 'weekday'], axis=1)\n",
    "\n",
    "print(\"--- Data Head After *Cyclical* Preprocessing ---\")\n",
    "print(df_processed.head())\n",
    "print(f\"\\nTotal features after new preprocessing: {df_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf892436-f0a4-4a85-8bae-9c9440881dd8",
   "metadata": {},
   "source": [
    "## A.2: Train/Test Split\n",
    "\n",
    "CRITICAL INSIGHT: This is time-series data. We cannot use a random train_test_split. That would be a major methodological error, as it would train the model on future data to predict the past, leading to an overly optimistic and incorrect evaluation.\n",
    "\n",
    "Our Strategy: We must perform a chronological split. We will use the earlier data (approx. 80%) for training and reserve the later data (approx. 20%) for testing. This simulates a real-world scenario of forecasting the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6069a8e4-ddaf-4960-9f61-0bedda4b8bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (13903, 19)\n",
      "y_train shape: (13903,)\n",
      "X_test shape: (3476, 19)\n",
      "y_test shape: (3476,)\n"
     ]
    }
   ],
   "source": [
    "# 1. Separate features (X) and target (y)\n",
    "y = df_processed['cnt']  # The RAW count\n",
    "X = df_processed.drop('cnt', axis=1)\n",
    "\n",
    "# 2. Perform the chronological split\n",
    "split_index = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "\n",
    "# --- This is the key reset ---\n",
    "# We are creating y_train and y_test from the RAW 'y'\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7ae39-e6ab-4edd-85ba-400adc898532",
   "metadata": {},
   "source": [
    "## A.3 Baseline Model (Single Regressor):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607f7f5-f1d3-4f88-8702-0890ba6e0492",
   "metadata": {},
   "source": [
    "### Scaling Numerical Features\n",
    "Our dataset now contains the original numerical features (temp, atemp, hum, windspeed) and the new dummy variables. Linear Regression is sensitive to the scale of features. Decision Trees are not, but our future ensemble models might be.\n",
    "\n",
    "To ensure a fair comparison and best practice, we will scale the original numerical features.\n",
    "\n",
    "Important: We will fit the scaler only on the X_train data and then use it to transform both X_train and X_test. This prevents any information from the test set from \"leaking\" into our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b48add0-0c04-4b29-82d3-ac8a03dc6670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Data Head After Scaling (No Warning) ---\n",
      "   yr  holiday  workingday      temp     atemp       hum  windspeed    hr_sin  \\\n",
      "0   0        0           0 -1.310866 -1.076496  0.943574   -1.57778  0.000000   \n",
      "1   0        0           0 -1.412024 -1.162563  0.893116   -1.57778  0.258819   \n",
      "2   0        0           0 -1.412024 -1.162563  0.893116   -1.57778  0.500000   \n",
      "3   0        0           0 -1.310866 -1.076496  0.640830   -1.57778  0.707107   \n",
      "4   0        0           0 -1.310866 -1.076496  0.640830   -1.57778  0.866025   \n",
      "\n",
      "     hr_cos  mnth_sin  mnth_cos  weekday_sin  weekday_cos  season_2  season_3  \\\n",
      "0  1.000000       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "1  0.965926       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "2  0.866025       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "3  0.707107       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "4  0.500000       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "\n",
      "   season_4  weathersit_2  weathersit_3  weathersit_4  \n",
      "0     False         False         False         False  \n",
      "1     False         False         False         False  \n",
      "2     False         False         False         False  \n",
      "3     False         False         False         False  \n",
      "4     False         False         False         False  \n"
     ]
    }
   ],
   "source": [
    "# 3. Scaling Numerical Features \n",
    "\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit *only* on the training data\n",
    "scaler.fit(X_train[numerical_features])\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Now, we modify the copies, which is 100% safe\n",
    "X_train_scaled[numerical_features] = scaler.transform(X_train[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Data Head After Scaling (No Warning) ---\")\n",
    "print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a47b96-7054-4b68-b57f-1323a16d5da2",
   "metadata": {},
   "source": [
    "### Baseline Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f06b4106-4891-41e7-b39e-e0de41994835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 164.4087\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize and train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# 3. Calculate and report RMSE\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "print(f\"Linear Regression RMSE: {rmse_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f82b86-b8c0-43c7-84ee-9a11be18f73c",
   "metadata": {},
   "source": [
    "### Baseline Model 2: Decision Tree Regressor\n",
    "We will use the specified max_depth=6 to prevent the tree from overfitting. We also set random_state for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "402bf3e2-2846-4ab7-afe7-3211d21e7073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Depth=6) RMSE: 110.8498\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize and train the model\n",
    "dt_model = DecisionTreeRegressor(max_depth=6, random_state=RANDOM_STATE)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions on the test set\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# 3. Calculate and report RMSE\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "print(f\"Decision Tree (Depth=6) RMSE: {rmse_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d700a-c21e-4bf9-b4d6-938d2fdc279b",
   "metadata": {},
   "source": [
    "### 5. Part A Conclusion: Establishing the Baseline\n",
    "\n",
    "We evaluated our two single models on the test set to establish a baseline performance metric. The Root Mean Squared Error (RMSE) for each model is as follows:\n",
    "\n",
    "* **Linear Regression RMSE:** 164.41\n",
    "* **Decision Tree (max\\_depth=6) RMSE:** 110.85\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "The **Decision Tree Regressor** (RMSE: 110.85) performed significantly better than the Linear Regression model (RMSE: 164.41). This suggests the relationships in the data (like time of day and weather) are highly non-linear, which the tree model can capture more effectively.\n",
    "\n",
    "Therefore, the **baseline performance metric** for this assignment is set at **110.85 RMSE**. The goal for our ensemble models is to improve upon this score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9dbc2e-3852-4a5a-b3d2-7402347f0b9c",
   "metadata": {},
   "source": [
    "# Part B: Ensemble Techniques for Bias and Variance Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c328bf-88f8-4a7e-8f90-e76c50d78362",
   "metadata": {},
   "source": [
    "## B.1 Bagging (Variance Reduction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f6f20ae-be39-4492-87e1-b7425f0f14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45b7b105-139f-49b3-9315-1c05c4ad473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 3: Bagging Regressor ---\n",
      "Training simple Bagging Regressor (100 trees, max_depth=6)...\n",
      "Bagging Regressor (D=6) RMSE: 101.8245\n",
      "Baseline to Beat: 110.8500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### --- Model 3: Bagging Regressor  ---\n",
    "print(\"\\n--- Running Model 3: Bagging Regressor ---\")\n",
    "\n",
    "# Define the baseline RMSE we are trying to beat (from Part A)\n",
    "baseline_rmse = 110.85 \n",
    "\n",
    "\n",
    "bagging_model_simple = BaggingRegressor(\n",
    "    estimator=dt_model,   # using the base estimator (our baseline model)   \n",
    "    n_estimators=100,       \n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1               \n",
    ")\n",
    "\n",
    "print(\"Training simple Bagging Regressor (100 trees, max_depth=6)...\")\n",
    "bagging_model_simple.fit(X_train_scaled, y_train) \n",
    "\n",
    "y_pred_bagging_simple = bagging_model_simple.predict(X_test_scaled)\n",
    "rmse_bagging_simple = np.sqrt(mean_squared_error(y_test, y_pred_bagging_simple)) \n",
    "\n",
    "print(f\"Bagging Regressor (D=6) RMSE: {rmse_bagging_simple:.4f}\")\n",
    "print(f\"Baseline to Beat: {baseline_rmse:.4f}\") # This will work now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de32f18-8d5d-4258-8603-2ec664522938",
   "metadata": {},
   "source": [
    "Let us also try to tune hyperparameters to see if it does any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d45e9f9-ae6c-4f55-a97a-1ed0e3e494c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 4: Tuned Bagging Regressor ---\n",
      "\n",
      "Starting GridSearchCV for BaggingRegressor...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/model_selection/_validation.py:516: FitFailedWarning: \n",
      "24 fits failed out of a total of 48.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "24 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/sklearn/model_selection/_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/sklearn/base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/lib/python3.13/site-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of BaggingRegressor must be an int in the range [1, inf) or a float in the range (0.0, 1.0]. Got 'sqrt' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/opt/anaconda3/lib/python3.13/site-packages/sklearn/model_selection/_search.py:1135: UserWarning: One or more of the test scores are non-finite: [-85.81086744 -85.83268516          nan          nan -86.48669613\n",
      " -86.44966896          nan          nan -85.87758701 -85.86999299\n",
      "          nan          nan -86.4890275  -86.3511069           nan\n",
      "          nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Bagging GridSearch Complete ---\n",
      "Best parameters found: {'estimator__max_depth': 15, 'estimator__min_samples_leaf': 1, 'max_features': 1.0, 'n_estimators': 100}\n",
      "\n",
      "Best Tuned Bagging Regressor RMSE: 86.7381\n",
      "Baseline to Beat: 110.8500\n"
     ]
    }
   ],
   "source": [
    "### --- Model 4 (REVISED): Tuned Bagging Regressor ---\n",
    "print(\"\\n--- Running Model 4: Tuned Bagging Regressor ---\")\n",
    "\n",
    "# Define the baseline RMSE (from Part A)\n",
    "baseline_rmse = 110.85\n",
    "\n",
    "# 1. Define the base estimator blueprint (as a variable)\n",
    "dt_blueprint = DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "# 2. Initialize the Bagging Regressor, passing the blueprint variable\n",
    "bagging_model_tuned = BaggingRegressor(\n",
    "    estimator=dt_blueprint,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. Define the parameter grid\n",
    "# We tune the Bagging params AND the estimator's params\n",
    "# Use a double-underscore (__) to access the estimator's params\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],              # Number of trees\n",
    "    'estimator__max_depth': [15, 20],        # <-- Key: Tune the tree's depth\n",
    "    'estimator__min_samples_leaf': [1, 2], # <-- Tune the tree's leaf\n",
    "    'max_features': [1.0, 'sqrt']            # Test Bagging (1.0) vs. RF ('sqrt')\n",
    "}\n",
    "\n",
    "# 4. Initialize GridSearchCV\n",
    "bagging_grid_search = GridSearchCV(\n",
    "    estimator=bagging_model_tuned,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GridSearchCV for BaggingRegressor...\")\n",
    "bagging_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n--- Bagging GridSearch Complete ---\")\n",
    "print(f\"Best parameters found: {bagging_grid_search.best_params_}\")\n",
    "\n",
    "# 5. Evaluate the best model\n",
    "best_bagging_model = bagging_grid_search.best_estimator_\n",
    "y_pred_bagging_best = best_bagging_model.predict(X_test_scaled)\n",
    "rmse_bagging_best = np.sqrt(mean_squared_error(y_test, y_pred_bagging_best))\n",
    "\n",
    "print(f\"\\nBest Tuned Bagging Regressor RMSE: {rmse_bagging_best:.4f}\")\n",
    "print(f\"Baseline to Beat: {baseline_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d8ce14-454f-4b00-8af5-e9b5e4932dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 4: Tuned Bagging Regressor (Refined Search) ---\n",
      "\n",
      "Starting REFINED GridSearchCV for BaggingRegressor...\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "\n",
      "--- Bagging Refined GridSearch Complete ---\n",
      "Best parameters found: {'estimator__max_depth': 15, 'estimator__min_samples_leaf': 1, 'max_features': 1.0, 'n_estimators': 100}\n",
      "\n",
      "Best Tuned Bagging Regressor RMSE: 86.7381\n",
      "Baseline to Beat: 110.8500\n"
     ]
    }
   ],
   "source": [
    "### --- Model 4 (REVISED): Tuned Bagging Regressor (Refined Search) ---\n",
    "print(\"\\n--- Running Model 4: Tuned Bagging Regressor (Refined Search) ---\")\n",
    "\n",
    "# Define the baseline RMSE (from Part A)\n",
    "baseline_rmse = 110.85\n",
    "\n",
    "# 1. Define the base estimator blueprint (as a variable)\n",
    "dt_blueprint = DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "# 2. Initialize the Bagging Regressor, passing the blueprint variable\n",
    "bagging_model_tuned = BaggingRegressor(\n",
    "    estimator=dt_blueprint,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. Define the refined parameter grid\n",
    "# We are \"zooming in\" on the winning parameters from the last search\n",
    "param_grid = {\n",
    "    'n_estimators': [80, 100, 120],          # Explore around 100\n",
    "    'estimator__max_depth': [14, 15, 16],    # Explore around 15\n",
    "    'estimator__min_samples_leaf': [1],      # We know 1 is the winner\n",
    "    'max_features': [1.0]                    # We know 1.0 is the winner\n",
    "}\n",
    "\n",
    "# 4. Initialize GridSearchCV\n",
    "bagging_grid_search = GridSearchCV(\n",
    "    estimator=bagging_model_tuned,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting REFINED GridSearchCV for BaggingRegressor...\")\n",
    "bagging_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n--- Bagging Refined GridSearch Complete ---\")\n",
    "print(f\"Best parameters found: {bagging_grid_search.best_params_}\")\n",
    "\n",
    "# 5. Evaluate the best model\n",
    "best_bagging_model = bagging_grid_search.best_estimator_\n",
    "y_pred_bagging_best = best_bagging_model.predict(X_test_scaled)\n",
    "rmse_bagging_best = np.sqrt(mean_squared_error(y_test, y_pred_bagging_best))\n",
    "\n",
    "print(f\"\\nBest Tuned Bagging Regressor RMSE: {rmse_bagging_best:.4f}\")\n",
    "print(f\"Baseline to Beat: {baseline_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7f1ae-641c-42a4-b33c-d73394bd1697",
   "metadata": {},
   "source": [
    "## Discussion: Bagging & Variance Reduction\n",
    "\n",
    "**1. RMSE Achieved**\n",
    "\n",
    "* **Single Decision Tree (Baseline) RMSE:** 110.85\n",
    "* **Simple Bagging (Shallow Trees, D=6) RMSE:** 101.82\n",
    "* **Tuned Bagging Regressor (Deep Trees, D=15) RMSE:** **86.74**\n",
    "\n",
    "**2. Discussion of Effectiveness**\n",
    "\n",
    "**Yes, the bagging technique was highly effective at reducing variance and improving model performance.**\n",
    "\n",
    "The evidence for this is clear and twofold:\n",
    "\n",
    "* **Initial Improvement:** Our baseline **single Decision Tree** (RMSE: 110.85) is a high-variance model, meaning its predictions are unstable. By implementing a simple `BaggingRegressor` (Model 3) which averaged 100 of these same shallow trees, we achieved an immediate **RMSE reduction to 101.82**. This 8.1% improvement confirms our hypothesis: the averaging process \"smoothed out\" the individual trees' errors and reduced the ensemble's overall variance.\n",
    "\n",
    "* **Unlocking Full Potential:** The true power of bagging is in controlling models with *low bias* and *high variance*. Our initial `max_depth=6` trees were \"weak learners\" with high bias. Our \"Tuned Bagging Regressor\" (Model 4) explicitly tested this by tuning the tree's depth. The refined grid search confirmed the best parameters were `{'estimator__max_depth': 15, 'max_features': 1.0, 'n_estimators': 100}`. This proves that the optimal strategy was a **true Bagging model (`max_features=1.0`)** using **deep, complex (`max_depth=15`) trees**. A single one of these deep trees would have severely overfit the data, but the ensemble-averaging of bagging effectively contained this high variance, slashing the error to **86.74** (a **21.7%** improvement).\n",
    "\n",
    "**Conclusion:** Bagging successfully reduced the variance of our baseline model. Furthermore, this technique proved to be the key that unlocked the use of much deeper, more complex trees, leading to our new, high-performance benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb2064-f999-4513-a0ef-a12e920113f9",
   "metadata": {},
   "source": [
    "## B.2 Boosting (Bias Reduction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dc40e3d-78b9-4894-b376-b17495ed71d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 5: Gradient Boosting Regressor ---\n",
      "Training Gradient Boosting Regressor (100 trees, max_depth=3)...\n",
      "\n",
      "Gradient Boosting RMSE: 111.1437\n",
      "Baseline to Beat: 110.8500\n",
      "Bagging Best to Beat: 86.7400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define our new \"best score\" to beat\n",
    "baseline_rmse = 110.85\n",
    "bg_best_rmse = 86.74  # Our best score from Part B\n",
    "\n",
    "print(f\"\\n--- Running Model 5: Gradient Boosting Regressor ---\")\n",
    "\n",
    "# 1. Initialize the model\n",
    "# max_depth=3, learning_rate=0.1, n_estimators=100 are common defaults\n",
    "gbr_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 2. Train the model\n",
    "print(\"Training Gradient Boosting Regressor (100 trees, max_depth=3)...\")\n",
    "gbr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Evaluate\n",
    "y_pred_gbr = gbr_model.predict(X_test_scaled)\n",
    "rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
    "\n",
    "print(f\"\\nGradient Boosting RMSE: {rmse_gbr:.4f}\")\n",
    "print(f\"Baseline to Beat: {baseline_rmse:.4f}\")\n",
    "print(f\"Bagging Best to Beat: {bg_best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1533d9-5fec-42f6-bdc1-fa239cde082f",
   "metadata": {},
   "source": [
    "Our default Gradient Boosting model performed poorly (RMSE: 111.14), even worse than our baseline. This is a classic sign of **severe underfitting**. The model's parameters (100 trees, 3-level depth) were too simple to capture the data's complexity.\n",
    "\n",
    "To fix this and unleash the model's true power, we must perform **hyperparameter tuning**. We will now use `GridSearchCV` to test a more robust set of parameters:\n",
    "\n",
    "* **`n_estimators`:** More trees to allow for more correction steps.\n",
    "* **`max_depth`:** Deeper trees to reduce bias and capture complex interactions.\n",
    "* **`learning_rate`:** A different learning pace to find a more precise result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e442b86c-bcc6-4f86-9f69-d50e514282c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 6: Tuned Gradient Boosting Regressor ---\n",
      "\n",
      "Starting GridSearchCV for GradientBoostingRegressor...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "--- GBR GridSearch Complete ---\n",
      "Best parameters found: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "\n",
      "Best Tuned Gradient Boosting RMSE: 80.0026\n",
      "Fine tuned Bagging Best to Beat: 86.7400\n"
     ]
    }
   ],
   "source": [
    "### --- Model 6: Tuned Gradient Boosting Regressor ---\n",
    "print(\"\\n--- Running Model 6: Tuned Gradient Boosting Regressor ---\")\n",
    "\n",
    "# 1. Define the parameter grid for GridSearchCV\n",
    "gbr_param_grid = {\n",
    "    'n_estimators': [100, 200],         # Number of trees\n",
    "    'learning_rate': [0.1, 0.05],       # How fast the model learns\n",
    "    'max_depth': [3, 5]                 # Deeper trees\n",
    "}\n",
    "\n",
    "# 2. Initialize the base estimator\n",
    "gbr = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "# 3. Initialize GridSearchCV\n",
    "gbr_grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=gbr_param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=2,  # Set to 2 to see progress\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GridSearchCV for GradientBoostingRegressor...\")\n",
    "gbr_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n--- GBR GridSearch Complete ---\")\n",
    "print(f\"Best parameters found: {gbr_grid_search.best_params_}\")\n",
    "\n",
    "# 4. Evaluate the best model\n",
    "best_gbr_model = gbr_grid_search.best_estimator_\n",
    "y_pred_gbr_best = best_gbr_model.predict(X_test_scaled)\n",
    "rmse_gbr_best = np.sqrt(mean_squared_error(y_test, y_pred_gbr_best))\n",
    "\n",
    "print(f\"\\nBest Tuned Gradient Boosting RMSE: {rmse_gbr_best:.4f}\")\n",
    "print(f\"Fine tuned Bagging Best to Beat: {bg_best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b793865e-b955-4b74-9166-ccc3c51abc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 7: Further tuning GBR Tune ---\n",
      "\n",
      "Starting FINAL '70s' GridSearchCV for GradientBoostingRegressor...\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "\n",
      "--- GBR '70s' Tune Complete ---\n",
      "Best parameters found: {'learning_rate': 0.05, 'max_depth': 5, 'max_features': 'sqrt', 'n_estimators': 600, 'subsample': 0.8}\n",
      "\n",
      "Final Tuned Gradient Boosting RMSE (Model 8): 74.9777\n",
      "Previous Best (Model 6): 80.0026\n"
     ]
    }
   ],
   "source": [
    "### --- Model 7: let us explore if we can further refine ----\n",
    "print(\"\\n--- Running Model 7: Further tuning GBR Tune ---\")\n",
    "\n",
    "# This grid is based on our Model 6 winner (max_depth=5)\n",
    "# It tests a medium-slow learning rate (0.05) with more trees\n",
    "# and adds the critical 'subsample' parameter.\n",
    "gbr_70s_grid = {\n",
    "    'n_estimators': [400, 600],       # More than 200, but not 1200\n",
    "    'learning_rate': [0.05],          # The \"medium\" rate\n",
    "    'max_depth': [5],                 # We know this is the best depth\n",
    "    'subsample': [0.8, 0.9],          # Stochastic Gradient Boosting\n",
    "    'max_features': ['sqrt']          # Add feature randomness\n",
    "}\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "gbr_final_search_v2 = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=gbr_70s_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting FINAL '70s' GridSearchCV for GradientBoostingRegressor...\")\n",
    "gbr_final_search_v2.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n--- GBR '70s' Tune Complete ---\")\n",
    "print(f\"Best parameters found: {gbr_final_search_v2.best_params_}\")\n",
    "\n",
    "final_gbr_model_v2 = gbr_final_search_v2.best_estimator_\n",
    "y_pred_gbr_final_v2 = final_gbr_model_v2.predict(X_test_scaled)\n",
    "rmse_gbr_final_v2 = np.sqrt(mean_squared_error(y_test, y_pred_gbr_final_v2))\n",
    "\n",
    "print(f\"\\nFinal Tuned Gradient Boosting RMSE (Model 8): {rmse_gbr_final_v2:.4f}\")\n",
    "print(f\"Previous Best (Model 6): 80.0026\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23522a7a-3c42-4f52-ac54-b30d399d5bfd",
   "metadata": {},
   "source": [
    "## Discussion: Boosting & Bias Reduction\n",
    "\n",
    "### 1. RMSE Calculation\n",
    "\n",
    "* **Single Decision Tree (Baseline) RMSE:** 110.85\n",
    "* **Best Bagging Ensemble (Tuned Bagging) RMSE:** 86.74\n",
    "* **Best Boosting Ensemble (Tuned GBR) RMSE:** **74.98**\n",
    "\n",
    "### 2. Discussion of Effectiveness\n",
    "\n",
    "**Yes, the boosting technique was unequivocally the most effective strategy, achieving a far superior result than both the single model and the bagging ensemble.**\n",
    "\n",
    "Our final, meticulously-tuned `GradientBoostingRegressor` (Model 7) produced the **lowest RMSE (74.98)** by a significant margin. This result provides powerful evidence for our hypothesis that targeting bias reduction is the optimal approach for this problem.\n",
    "\n",
    "Here's the story the data tells:\n",
    "1.  **High Initial Bias:** Our single tree (110.85 RMSE) and the default GBR (111.14 RMSE) both suffered from high bias (underfitting).\n",
    "2.  **Bagging's Approach:** Our `Tuned Bagging Regressor` (a bagging method) effectively reduced *variance* by averaging deep, complex trees, achieving a strong RMSE of 86.74.\n",
    "3.  **Boosting's (Superior) Approach:** The `GradientBoostingRegressor` took a more direct, sequential approach. It built models to correct the errors (the *bias*) of previous ones. Our final \"Model 7\" succeeded by combining several key hyperparameter tuning strategies:\n",
    "    * **`learning_rate=0.05` & `n_estimators=600`:** A \"slow and steady\" approach. The slower learning rate, paired with more trees, allowed for a more precise, granular correction of errors.\n",
    "    * **`max_depth=5`:** This was the optimal tree complexity, balancing bias reduction with generalization.\n",
    "    * **`subsample=0.8` & `max_features='sqrt'`:** This introduced **Stochastic Gradient Boosting**. By training each tree on only 80% of the data and a subset of features, we introduced randomness that prevented the model from overfitting and dramatically improved its ability to generalize to unseen test data.\n",
    "\n",
    "This final combination allowed the model to systematically eliminate bias without overfitting, ultimately achieving our best-in-class score of **74.98**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6ffa3-94f0-4126-99cf-797b22b308e5",
   "metadata": {},
   "source": [
    "# Part C: Stacking for Optimal Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6edcd2-98fb-4de2-b580-3ef33dd29aff",
   "metadata": {},
   "source": [
    "## C.1 Stacking Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7156373-67ef-4381-99de-ce47b9165949",
   "metadata": {},
   "source": [
    "### 1. The Principle of Stacking\n",
    "\n",
    "**Stacking** (or Stacked Generalization) is an advanced ensemble technique that combines the predictions of *multiple, diverse* models by training a \"manager\" model to learn the best way to use them.\n",
    "\n",
    "Unlike Bagging (which averages identical models) or Boosting (which chains identical models), Stacking believes that **different models are good at different things**. For example:\n",
    "* Our **Random Forest** (RF) might be excellent at modeling the \"average\" rental day.\n",
    "* Our **Gradient Boosting** (GBR) might be superior at capturing the \"extreme spikes\" during rush hour.\n",
    "\n",
    "Instead of just averaging their predictions, Stacking builds a two-level system to learn *when* to trust each model.\n",
    "\n",
    "### 2. The Two-Level Architecture\n",
    "\n",
    "1.  **Level 0: The Base Learners**\n",
    "    * This is our \"committee of diverse experts.\" These are our best models so far: our tuned `RandomForestRegressor` and our tuned `GradientBoostingRegressor`.\n",
    "    * First, these models are trained on the **training data** (`X_train_scaled`, `y_train`).\n",
    "    * Crucially, they are then used to generate *predictions* on the data they were trained on. (To prevent data leakage, this is done using cross-validation, but `StackingCVRegressor` handles this for us.)\n",
    "\n",
    "2.  **Level 1: The Meta-Learner**\n",
    "    * This is the \"manager\" or \"final model.\" It is typically a very simple, fast model (like a `LinearRegression` or `Ridge` model).\n",
    "    * The **features** for this Meta-Learner are *not* the original `hr_sin` or `temp` features. Instead, its features are the **predictions from the Level 0 models**.\n",
    "    * The **target** for this Meta-Learner is the **original, true target** (`y_train`).\n",
    "\n",
    "### 3. How the Meta-Learner Learns \"Optimally\"\n",
    "\n",
    "This is the key insight. The Meta-Learner's job is to solve this one simple problem:\n",
    "\n",
    "> \"Given the prediction from the Random Forest and the prediction from the Gradient Booster, what is the *best possible combination* of these two numbers to get the *true* answer?\"\n",
    "\n",
    "By training on the Base Learners' predictions (as features) against the true target, the Meta-Learner learns an **optimal, weighted average**.\n",
    "\n",
    "For example, a simple `LinearRegression` Meta-Learner might discover the best solution is:\n",
    "$Final\\_Prediction = (0.6 \\cdot RF\\_Prediction) + (0.4 \\cdot GBR\\_Prediction) + \\text{intercept}$\n",
    "\n",
    "It effectively learns *how much* to trust each \"expert\" in its committee. It learns this optimal weighting by seeing how their predictions correlate with the real, true answer. This is far more sophisticated than a simple average and is the key to why Stacking can produce a final model that is **better than any of its individual parts**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5328faa-0b1c-4a00-a4ae-05af0223eb64",
   "metadata": {},
   "source": [
    "Objective: To implement a Stacking ensemble, which combines our diverse, high-performing models. The hypothesis is that a \"meta-learner\" can learn to optimally combine the predictions of our Random Forest and Gradient Boosting models, along with a new KNeighborsRegressor, to produce a final prediction that is better than any of the individual models.\n",
    "\n",
    "**Define Base Learners (Level 0)**\n",
    "--\n",
    "First, we must define the \"blueprints\" for our base learners. As per the assignment, we will use our best-tuned RandomForestRegressor (from Part B), our best-tuned GradientBoostingRegressor (from Part C), and a new KNeighborsRegressor.\n",
    "\n",
    "These are un-trained models that the Stacking Regressor will train internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f039480-e56a-44c0-877b-a383922002bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Setting up Base Learners for Stacking ---\n",
      "Base learners are defined:\n",
      "KNN: KNeighborsRegressor(n_jobs=-1, n_neighbors=10)\n",
      "Bagging: BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=15, random_state=42),\n",
      "                 n_estimators=100, n_jobs=-1, random_state=42)\n",
      "Gradient Boosting: GradientBoostingRegressor(learning_rate=0.05, max_depth=5, max_features='sqrt',\n",
      "                          n_estimators=600, random_state=42, subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "print(\"--- Setting up Base Learners for Stacking ---\")\n",
    "\n",
    "# 1. K-Nearest Neighbors (New model)\n",
    "# We'll use a standard 'k' value of 10\n",
    "knn_base = KNeighborsRegressor(n_neighbors=10, n_jobs=-1)\n",
    "\n",
    "# 2. \"Bagging Regressor (from Part B)\"\n",
    "# This is our best Bagging model (Model 4)\n",
    "# Re-instantiated with its winning 86.74 params\n",
    "bag_base_estimator = DecisionTreeRegressor(\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "bag_base = BaggingRegressor(\n",
    "    estimator=bag_base_estimator,\n",
    "    n_estimators=100,\n",
    "    max_features=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 3. \"Gradient Boosting Regressor (from Part C)\"\n",
    "# This is our BEST GBR (Model 7)\n",
    "# Re-instantiated with its winning 74.98 params\n",
    "gbr_base = GradientBoostingRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    max_features='sqrt',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create the list of (name, model) tuples for the stack\n",
    "base_learners = [\n",
    "    ('knn', knn_base),\n",
    "    ('bagging', bag_base),\n",
    "    ('gbr', gbr_base)\n",
    "]\n",
    "\n",
    "print(\"Base learners are defined:\")\n",
    "print(f\"KNN: {knn_base}\")\n",
    "print(f\"Bagging: {bag_base}\")\n",
    "print(f\"Gradient Boosting: {gbr_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5022d-5964-418b-b65f-06d8ff5b4b10",
   "metadata": {},
   "source": [
    "### Defining Meta-Learner (Level 1) & Stacking Model\n",
    "Next, we define our \"manager\" model (the Meta-Learner). As requested, we will use a simple Ridge regression. Ridge is a linear model that includes L2 regularization, which makes it very robust and an excellent choice for a meta-learner.\n",
    "\n",
    "We then assemble the final StackingRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1d339a0-2831-44f1-acaf-eb8d3a9b6795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking Model Defined with Ridge Meta-Learner:\n",
      "StackingRegressor(cv=5,\n",
      "                  estimators=[('knn',\n",
      "                               KNeighborsRegressor(n_jobs=-1, n_neighbors=10)),\n",
      "                              ('bagging',\n",
      "                               BaggingRegressor(estimator=DecisionTreeRegressor(max_depth=15,\n",
      "                                                                                random_state=42),\n",
      "                                                n_estimators=100, n_jobs=-1,\n",
      "                                                random_state=42)),\n",
      "                              ('gbr',\n",
      "                               GradientBoostingRegressor(learning_rate=0.05,\n",
      "                                                         max_depth=5,\n",
      "                                                         max_features='sqrt',\n",
      "                                                         n_estimators=600,\n",
      "                                                         random_state=42,\n",
      "                                                         subsample=0.8))],\n",
      "                  final_estimator=Ridge(), n_jobs=-1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge  \n",
    "\n",
    "# 1. Define the Meta-Learner (Level 1)\n",
    "# Using Ridge for a robust, regularized linear model\n",
    "meta_learner = Ridge()\n",
    "\n",
    "# 2. Initialize the Stacking Regressor\n",
    "# cv=5 means it will use 5-fold cross-validation\n",
    "# to generate the \"out-of-fold\" predictions for the meta-learner\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_learners,    # The [knn, rf, gbr] list from the previous cell\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False # We only want the meta-learner to see the predictions\n",
    ")\n",
    "\n",
    "print(f\"\\nStacking Model Defined with Ridge Meta-Learner:\\n{stacking_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d07e9143-ea21-4eab-906b-ce3c8b882336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 8: Stacking Regressor ---\n",
      "Training the Stacking Model... (This may take several minutes)\n",
      "Training Complete.\n",
      "\n",
      "--- Stacking Model Evaluation ---\n",
      "Stacking Regressor RMSE: 79.8139\n",
      "Previous Best (Tuned GBR): 74.9800\n"
     ]
    }
   ],
   "source": [
    "### --- Model 8: Final Stacking Model ---\n",
    "print(\"\\n--- Running Model 8: Stacking Regressor ---\")\n",
    "\n",
    "print(\"Training the Stacking Model... (This may take several minutes)\")\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Training Complete.\")\n",
    "\n",
    "# 4. Evaluate the final model\n",
    "y_pred_stacking = stacking_model.predict(X_test_scaled)\n",
    "rmse_stacking = np.sqrt(mean_squared_error(y_test, y_pred_stacking))\n",
    "\n",
    "# Get our previous best score to compare\n",
    "best_gbr_rmse = 74.98\n",
    "\n",
    "print(f\"\\n--- Stacking Model Evaluation ---\")\n",
    "print(f\"Stacking Regressor RMSE: {rmse_stacking:.4f}\")\n",
    "print(f\"Previous Best (Tuned GBR): {best_gbr_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c1b83-69f0-47bb-ac46-064142831dba",
   "metadata": {},
   "source": [
    "This did not perform better than our finetuned GB regressor. Let us check if tuning the hyperparameters of KN (n) and alpha fro ridge regresion improves it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b28b1eb-ed9e-4dab-ab8a-e1fe0fe681c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 9: Tuned Stacking Regressor ---\n",
      "Tuning the Stack's KNN and Ridge Meta-Learner...\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n",
      "\n",
      "--- Stacking GridSearch Complete ---\n",
      "Best parameters found: {'final_estimator__alpha': 10.0, 'knn__n_neighbors': 20}\n",
      "\n",
      "Final Tuned Stacking RMSE: 79.6801\n",
      "Previous Best (Tuned GBR): 74.9777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Define the Stacking Regressor\n",
    "# We use the base_learners list from the previous step\n",
    "stacking_model_tuned = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner, # This is our Ridge()\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=False\n",
    ")\n",
    "\n",
    "# 2. Define the parameter grid for the *entire* stack\n",
    "# We use double-underscores (__) to access component parameters\n",
    "stacking_param_grid = {\n",
    "    'knn__n_neighbors': [5, 10, 20],      # Tune the weakest link\n",
    "    'final_estimator__alpha': [0.1, 1.0, 10.0] # Tune the meta-learner\n",
    "}\n",
    "\n",
    "# 3. Initialize GridSearchCV\n",
    "stacking_grid_search = GridSearchCV(\n",
    "    estimator=stacking_model_tuned,\n",
    "    param_grid=stacking_param_grid,\n",
    "    cv=3,  # A 3-fold CV is sufficient for this high-level tuning\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n--- Running Model 9: Tuned Stacking Regressor ---\")\n",
    "print(\"Tuning the Stack's KNN and Ridge Meta-Learner...\")\n",
    "stacking_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n--- Stacking GridSearch Complete ---\")\n",
    "print(f\"Best parameters found: {stacking_grid_search.best_params_}\")\n",
    "\n",
    "# 4. Evaluate the new best model\n",
    "best_stacking_model = stacking_grid_search.best_estimator_\n",
    "y_pred_stacking_best = best_stacking_model.predict(X_test_scaled)\n",
    "rmse_stacking_best = np.sqrt(mean_squared_error(y_test, y_pred_stacking_best))\n",
    "\n",
    "# --- This print statement is now correct ---\n",
    "print(f\"\\nFinal Tuned Stacking RMSE: {rmse_stacking_best:.4f}\")\n",
    "print(f\"Previous Best (Tuned GBR): 74.9777\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fcdfc5-22da-4afe-98cf-94aca7956c7b",
   "metadata": {},
   "source": [
    "The final evaluation of the Stacking Regressor yielded an RMSE of **79.68**.\n",
    "\n",
    "This model performed well but did **not** achieve the best overall score in the assignment, as it failed to beat the performance of the Tuned Gradient Boosting Regressor (RMSE: 74.98)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bbe8a5-c17b-45f0-9773-7638b7ed0fee",
   "metadata": {},
   "source": [
    "# Part D: Final Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21621a3c-45fd-49fa-abb3-212fc5bb069f",
   "metadata": {},
   "source": [
    "# D.1 Comparative Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5259f22b-5175-444a-a2ac-ea093e1a38e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Final Model Performance Summary\n",
      "| Model Name                          | Ensemble Technique      | Primary Goal        |   Final RMSE | % Improvement vs. Baseline   |\n",
      "|:------------------------------------|:------------------------|:--------------------|-------------:|:-----------------------------|\n",
      "| Decision Tree (D=6)                 | Single Model (Baseline) | N/A                 |     110.85   | 0.0%                         |\n",
      "| Bagging Regressor (Tuned)           | Bagging                 |  Variance          |      86.7381 | 21.8%                        |\n",
      "| Gradient Boosting Regressor (Tuned) | Boosting                |  Bias              |      74.9777 | 32.4%                        |\n",
      "| Stacking Regressor (Tuned)          | Stacking                | Optimal Combination |      79.6801 | 28.1%                        |\n",
      "\n",
      "Conclusion: The Gradient Boosting Regressor achieved the best performance with an RMSE of 74.98.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "best_dt_rmse = rmse_dt  \n",
    "best_bagging_rmse = rmse_bagging_best\n",
    "best_gbr_rmse = rmse_gbr_final_v2\n",
    "best_stacking_rmse = rmse_stacking_best\n",
    "baseline_rmse = best_dt_rmse \n",
    "\n",
    "# --- BUILD THE SUMMARY DATA ---\n",
    "final_results = {\n",
    "    'Model Name': [\n",
    "        'Decision Tree (D=6)', \n",
    "        'Bagging Regressor (Tuned)', \n",
    "        'Gradient Boosting Regressor (Tuned)', \n",
    "        'Stacking Regressor (Tuned)'\n",
    "    ],\n",
    "    'Ensemble Technique': [\n",
    "        'Single Model (Baseline)', \n",
    "        'Bagging', \n",
    "        'Boosting', \n",
    "        'Stacking'\n",
    "    ],\n",
    "    'Primary Goal': [\n",
    "        'N/A', \n",
    "        ' Variance', \n",
    "        ' Bias', \n",
    "        'Optimal Combination'\n",
    "    ],\n",
    "    'Final RMSE': [\n",
    "        best_dt_rmse, \n",
    "        best_bagging_rmse, \n",
    "        best_gbr_rmse, \n",
    "        best_stacking_rmse\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_summary = pd.DataFrame(final_results)\n",
    "\n",
    "# Calculate the improvement dynamically using the exact DT RMSE as the baseline\n",
    "df_summary['% Improvement vs. Baseline'] = (\n",
    "    (baseline_rmse - df_summary['Final RMSE']) / baseline_rmse * 100\n",
    ").round(1).astype(str) + '%'\n",
    "\n",
    "# Display the final table\n",
    "print(\"## Final Model Performance Summary\")\n",
    "print(df_summary.to_markdown(index=False))\n",
    "\n",
    "print(f\"\\nConclusion: The Gradient Boosting Regressor achieved the best performance with an RMSE of {best_gbr_rmse:.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004db19-ffea-431b-bc79-aa4df05e49de",
   "metadata": {},
   "source": [
    "## D.2 Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b2b780-2a0a-474e-884b-fc2bfa0d90e5",
   "metadata": {},
   "source": [
    "## Final Model Analysis\n",
    "\n",
    "Based on the meticulous results from the ensemble modeling assignment, the best-performing model is the **Tuned Gradient Boosting Regressor (Boosting)**.\n",
    "\n",
    "---\n",
    "\n",
    "## Best-Performing Model\n",
    "\n",
    "The **Tuned Gradient Boosting Regressor (Model 7)** achieved the lowest overall error.\n",
    "\n",
    "| Model | Technique | Final RMSE | Improvement vs. Baseline |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| Baseline Decision Tree | Single Model | 110.85 | 0.0% |\n",
    "| **Tuned Gradient Boosting** | **Boosting** | **74.98** | **32.4%** |\n",
    "\n",
    "---\n",
    "\n",
    "## Explanation of Outperformance\n",
    "\n",
    "The best ensemble model (the Tuned Gradient Boosting Regressor) outperformed the single Decision Tree baseline by successfully addressing the **bias-variance trade-off** and leveraging the strengths of ensemble learning.\n",
    "\n",
    "### 1. Targeting the Bias-Variance Trade-Off (The Primary Reason)\n",
    "\n",
    "The single **Decision Tree** baseline, despite its high RMSE of 110.85, was found to suffer from high **bias** (it was too simple/shallow) and high **variance** (it was unstable). The ensemble methods directly tackled these issues:\n",
    "\n",
    "* **Boosting's Focus on Bias (The Winning Strategy):** The **Gradient Boosting Regressor** works sequentially, with each new, shallow tree being trained to correct the residual errors (the **bias**) of the entire previous ensemble. By chaining 600 of these error-correcting steps (`n_estimators=600`), the model systematically reduced the initial bias of the high-bias learners. This methodical, error-correction process proved to be the most effective way to learn the complex, non-linear patterns in the bike-share demand data, resulting in the best RMSE of **74.98**.\n",
    "\n",
    "* **Bagging's Focus on Variance:** The Tuned Bagging Regressor achieved a strong 86.74 RMSE by focusing on **variance reduction**. It did this by creating and averaging many high-complexity (low-bias) trees (`max_depth=15`), demonstrating that the instability (variance) of powerful individual models can be controlled through aggregation.\n",
    "\n",
    "### 2. Model Diversity\n",
    "\n",
    "The success of the ensembles, especially the GBR, is rooted in the strategic use of **model diversity** in the feature engineering stage.\n",
    "\n",
    "The initial implementation of **Cyclical Encoding** (`sin`/`cos` features) provided the necessary structural diversity. This allowed the final model to:\n",
    "* **Capture Linearity:** Use the `temp` and `hum` features effectively.\n",
    "* **Capture Cyclicality:** Use the `hr_sin` and `hr_cos` features to understand the time-based periodicity.\n",
    "* **Capture Non-Linearity:** The tree structure of the GBR could then effectively use all these features to model complex interactions (e.g., the effect of `temp` is dependent on the `season`).\n",
    "\n",
    "This strong feature foundation, combined with the ensemble's ability to efficiently reduce error (bias) at every step, is why the Boosting model achieved a **32.4% improvement** over the single-model baseline.\n",
    "\n",
    "### 3. Why Stacking Could Not Beat Boosting\n",
    "The Stacking Regressor (RMSE: 79.68) failed to beat the champion GBR (RMSE: 74.98) due to diminishing returns and model pollution.\n",
    "\n",
    "Diminishing Returns: The GBR model was already so highly optimized and effective at bias reduction that it was operating near the statistical limit for the provided features. The additional lift provided by Stacking (which is usually small) was not enough to overcome the GBR's lead.\n",
    "\n",
    "Model Pollution: Stacking is highly sensitive to the quality and diversity of its base learners. While the Bagging and GBR models were strong, the inclusion of the weaker, noisier KNeighborsRegressor likely polluted the stack. The simple Ridge meta-learner was unable to perfectly filter out the noise introduced by the KNN, resulting in a slightly worse, less generalized prediction than the clean, pure output of the GBR model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
