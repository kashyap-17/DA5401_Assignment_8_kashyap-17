{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7740dc34-61e9-481e-a010-4ae729cac03c",
   "metadata": {},
   "source": [
    "# Part A Data Preprocessing and Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4b7bf-4b70-43ff-baad-041a9ec57095",
   "metadata": {},
   "source": [
    "## A.1 Data Loading and Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2564df0c-d422-4ad5-bb2c-738d3fe3c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set a random state for reproducibility\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aeab32-5758-43cb-884f-e552872e255b",
   "metadata": {},
   "source": [
    "We'll load the hour.csv file and take a first look at its structure, data types, and any potential missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34eb8630-0242-4c10-96e7-0046dd57655d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Head ---\n",
      "   instant      dteday  season  yr  mnth  hr  holiday  weekday  workingday  \\\n",
      "0        1  2011-01-01       1   0     1   0        0        6           0   \n",
      "1        2  2011-01-01       1   0     1   1        0        6           0   \n",
      "2        3  2011-01-01       1   0     1   2        0        6           0   \n",
      "3        4  2011-01-01       1   0     1   3        0        6           0   \n",
      "4        5  2011-01-01       1   0     1   4        0        6           0   \n",
      "\n",
      "   weathersit  temp   atemp   hum  windspeed  casual  registered  cnt  \n",
      "0           1  0.24  0.2879  0.81        0.0       3          13   16  \n",
      "1           1  0.22  0.2727  0.80        0.0       8          32   40  \n",
      "2           1  0.22  0.2727  0.80        0.0       5          27   32  \n",
      "3           1  0.24  0.2879  0.75        0.0       3          10   13  \n",
      "4           1  0.24  0.2879  0.75        0.0       0           1    1  \n",
      "\n",
      "--- Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17379 entries, 0 to 17378\n",
      "Data columns (total 17 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   instant     17379 non-null  int64  \n",
      " 1   dteday      17379 non-null  object \n",
      " 2   season      17379 non-null  int64  \n",
      " 3   yr          17379 non-null  int64  \n",
      " 4   mnth        17379 non-null  int64  \n",
      " 5   hr          17379 non-null  int64  \n",
      " 6   holiday     17379 non-null  int64  \n",
      " 7   weekday     17379 non-null  int64  \n",
      " 8   workingday  17379 non-null  int64  \n",
      " 9   weathersit  17379 non-null  int64  \n",
      " 10  temp        17379 non-null  float64\n",
      " 11  atemp       17379 non-null  float64\n",
      " 12  hum         17379 non-null  float64\n",
      " 13  windspeed   17379 non-null  float64\n",
      " 14  casual      17379 non-null  int64  \n",
      " 15  registered  17379 non-null  int64  \n",
      " 16  cnt         17379 non-null  int64  \n",
      "dtypes: float64(4), int64(12), object(1)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df_hour = pd.read_csv('hour.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'hour.csv' not found. Please download it from the UCI repository.\")\n",
    "\n",
    "print(\"--- Data Head ---\")\n",
    "print(df_hour.head())\n",
    "print(\"\\n--- Data Info ---\")\n",
    "df_hour.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d92b8a2-ef54-4413-8cb3-23b6a35ef9f8",
   "metadata": {},
   "source": [
    "The data is loaded successfully. We have 17,379 hourly entries.\n",
    "\n",
    "There are no missing values, as indicated by the info() output.\n",
    "\n",
    "dteday is an object (string), which we'll be dropping.\n",
    "\n",
    "Most features are int64 or float64, which is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f725bbb-d6c3-4b12-a534-aa991bb3ffa7",
   "metadata": {},
   "source": [
    "### Dropping Irrelevant & Leakage Columns\n",
    "As per the assignment, we will drop instant, dteday, casual, and registered.\n",
    "\n",
    "instant: This is just a row index and provides no predictive information.\n",
    "\n",
    "dteday: This date column is redundant. All its information (year, month, day) is already captured in yr, mnth, weekday, and hr.\n",
    "\n",
    "casual and registered: This is the most critical drop. These two columns sum up to our target variable, cnt. Including them would be data leakage, leading to a perfect but useless model (it's like predicting a total score when you already know the component scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b3cf9e-b807-4c6c-940c-8a080538554f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original columns: 17\n",
      "Processed columns: 13\n",
      "Dropped 'instant', 'dteday', 'casual', 'registered'.\n"
     ]
    }
   ],
   "source": [
    "# Drop the specified columns\n",
    "df_processed = df_hour.drop(['instant', 'dteday', 'casual', 'registered'], axis=1)\n",
    "\n",
    "print(f\"\\nOriginal columns: {df_hour.shape[1]}\")\n",
    "print(f\"Processed columns: {df_processed.shape[1]}\")\n",
    "print(\"Dropped 'instant', 'dteday', 'casual', 'registered'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bb3ab-9b08-40ff-9ccd-d4e6aae13c06",
   "metadata": {},
   "source": [
    "### Feature Engineering: A Hybrid Approach\n",
    "\n",
    "Next, we convert our categorical features into a numerical format. This is the most critical step for model performance, and we must be precise. Our features fall into two distinct groups:\n",
    "\n",
    "1.  **Cyclical Features:** `hr`, `mnth`, and `weekday`.\n",
    "2.  **Nominal Features:** `season` and `weathersit`.\n",
    "\n",
    "A common mistake is to use One-Hot Encoding (OHE) for all of them. However, that would make our model *worse*. For example, OHE treats \"hour 0\" (midnight) and \"hour 23\" (11 PM) as completely unrelated, when in reality, they are adjacent.\n",
    "\n",
    "**Our Expert Strategy:**\n",
    "\n",
    "* **Cyclical Encoding (`sin`/`cos`):** For `hr`, `mnth`, and `weekday`, we will use a mathematical transformation. By converting them into `sin` and `cos` components, we map them onto a circle. This explicitly tells the model that \"hour 23\" is very close to \"hour 0,\" and \"month 12\" is very close to \"month 1.\" This should provide a massive boost in predictive power.\n",
    "* **One-Hot Encoding (`pd.get_dummies`):** For `season` and `weathersit`, which are nominal (unordered categories), OHE remains the correct approach. We'll set `drop_first=True` to prevent multicollinearity (the \"dummy variable trap\"), which is important for our Linear Regression baseline.\n",
    "\n",
    "This hybrid approach gives our models the best possible representation of the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eaca3ca-762f-4257-8b2f-3039bfbd35a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Head After *Cyclical* Preprocessing ---\n",
      "   yr  holiday  workingday  temp   atemp   hum  windspeed  cnt    hr_sin  \\\n",
      "0   0        0           0  0.24  0.2879  0.81        0.0   16  0.000000   \n",
      "1   0        0           0  0.22  0.2727  0.80        0.0   40  0.258819   \n",
      "2   0        0           0  0.22  0.2727  0.80        0.0   32  0.500000   \n",
      "3   0        0           0  0.24  0.2879  0.75        0.0   13  0.707107   \n",
      "4   0        0           0  0.24  0.2879  0.75        0.0    1  0.866025   \n",
      "\n",
      "     hr_cos  mnth_sin  mnth_cos  weekday_sin  weekday_cos  season_2  season_3  \\\n",
      "0  1.000000       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "1  0.965926       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "2  0.866025       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "3  0.707107       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "4  0.500000       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "\n",
      "   season_4  weathersit_2  weathersit_3  weathersit_4  \n",
      "0     False         False         False         False  \n",
      "1     False         False         False         False  \n",
      "2     False         False         False         False  \n",
      "3     False         False         False         False  \n",
      "4     False         False         False         False  \n",
      "\n",
      "Total features after new preprocessing: 20\n"
     ]
    }
   ],
   "source": [
    "# --- Cyclical Feature Engineering ---\n",
    "# Apply sin/cos transformations to cyclical features\n",
    "\n",
    "# Handle 'hr' (0-23, so a 24-hour cycle)\n",
    "df_processed['hr_sin'] = np.sin(2 * np.pi * df_processed['hr'] / 24.0)\n",
    "df_processed['hr_cos'] = np.cos(2 * np.pi * df_processed['hr'] / 24.0)\n",
    "\n",
    "# Handle 'mnth' (1-12, so a 12-month cycle)\n",
    "df_processed['mnth_sin'] = np.sin(2 * np.pi * df_processed['mnth'] / 12.0)\n",
    "df_processed['mnth_cos'] = np.cos(2 * np.pi * df_processed['mnth'] / 12.0)\n",
    "\n",
    "# Handle 'weekday' (0-6, so a 7-day cycle)\n",
    "df_processed['weekday_sin'] = np.sin(2 * np.pi * df_processed['weekday'] / 7.0)\n",
    "df_processed['weekday_cos'] = np.cos(2 * np.pi * df_processed['weekday'] / 7.0)\n",
    "\n",
    "# --- One-Hot Encoding for *non-cyclical* categories ---\n",
    "# 'season' and 'weathersit' are nominal, so OHE is still correct here.\n",
    "non_cyclical_cats = ['season', 'weathersit']\n",
    "df_processed = pd.get_dummies(df_processed, \n",
    "                              columns=non_cyclical_cats, \n",
    "                              drop_first=True)\n",
    "\n",
    "# Now we can drop the original categorical columns\n",
    "df_processed = df_processed.drop(['hr', 'mnth', 'weekday'], axis=1)\n",
    "\n",
    "print(\"--- Data Head After *Cyclical* Preprocessing ---\")\n",
    "print(df_processed.head())\n",
    "print(f\"\\nTotal features after new preprocessing: {df_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf892436-f0a4-4a85-8bae-9c9440881dd8",
   "metadata": {},
   "source": [
    "## A.2: Train/Test Split\n",
    "\n",
    "CRITICAL INSIGHT: This is time-series data. We cannot use a random train_test_split. That would be a major methodological error, as it would train the model on future data to predict the past, leading to an overly optimistic and incorrect evaluation.\n",
    "\n",
    "Our Strategy: We must perform a chronological split. We will use the earlier data (approx. 80%) for training and reserve the later data (approx. 20%) for testing. This simulates a real-world scenario of forecasting the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6069a8e4-ddaf-4960-9f61-0bedda4b8bf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (13903, 19)\n",
      "y_train shape: (13903,)\n",
      "X_test shape: (3476, 19)\n",
      "y_test shape: (3476,)\n"
     ]
    }
   ],
   "source": [
    "# 1. Separate features (X) and target (y)\n",
    "y = df_processed['cnt']  # The RAW count\n",
    "X = df_processed.drop('cnt', axis=1)\n",
    "\n",
    "# 2. Perform the chronological split\n",
    "split_index = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_index]\n",
    "X_test = X.iloc[split_index:]\n",
    "\n",
    "# --- This is the key reset ---\n",
    "# We are creating y_train and y_test from the RAW 'y'\n",
    "y_train = y.iloc[:split_index]\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7ae39-e6ab-4edd-85ba-400adc898532",
   "metadata": {},
   "source": [
    "## A.3 Baseline Model (Single Regressor):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3607f7f5-f1d3-4f88-8702-0890ba6e0492",
   "metadata": {},
   "source": [
    "### Scaling Numerical Features\n",
    "Our dataset now contains the original numerical features (temp, atemp, hum, windspeed) and the new dummy variables. Linear Regression is sensitive to the scale of features. Decision Trees are not, but our future ensemble models might be.\n",
    "\n",
    "To ensure a fair comparison and best practice, we will scale the original numerical features.\n",
    "\n",
    "Important: We will fit the scaler only on the X_train data and then use it to transform both X_train and X_test. This prevents any information from the test set from \"leaking\" into our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b48add0-0c04-4b29-82d3-ac8a03dc6670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Data Head After Scaling (No Warning) ---\n",
      "   yr  holiday  workingday      temp     atemp       hum  windspeed    hr_sin  \\\n",
      "0   0        0           0 -1.310866 -1.076496  0.943574   -1.57778  0.000000   \n",
      "1   0        0           0 -1.412024 -1.162563  0.893116   -1.57778  0.258819   \n",
      "2   0        0           0 -1.412024 -1.162563  0.893116   -1.57778  0.500000   \n",
      "3   0        0           0 -1.310866 -1.076496  0.640830   -1.57778  0.707107   \n",
      "4   0        0           0 -1.310866 -1.076496  0.640830   -1.57778  0.866025   \n",
      "\n",
      "     hr_cos  mnth_sin  mnth_cos  weekday_sin  weekday_cos  season_2  season_3  \\\n",
      "0  1.000000       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "1  0.965926       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "2  0.866025       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "3  0.707107       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "4  0.500000       0.5  0.866025    -0.781831      0.62349     False     False   \n",
      "\n",
      "   season_4  weathersit_2  weathersit_3  weathersit_4  \n",
      "0     False         False         False         False  \n",
      "1     False         False         False         False  \n",
      "2     False         False         False         False  \n",
      "3     False         False         False         False  \n",
      "4     False         False         False         False  \n"
     ]
    }
   ],
   "source": [
    "# 3. Scaling Numerical Features \n",
    "\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit *only* on the training data\n",
    "scaler.fit(X_train[numerical_features])\n",
    "\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "# Now, we modify the copies, which is 100% safe\n",
    "X_train_scaled[numerical_features] = scaler.transform(X_train[numerical_features])\n",
    "X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Data Head After Scaling (No Warning) ---\")\n",
    "print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a47b96-7054-4b68-b57f-1323a16d5da2",
   "metadata": {},
   "source": [
    "### Baseline Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f06b4106-4891-41e7-b39e-e0de41994835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RMSE: 164.4087\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize and train the model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions on the test set\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# 3. Calculate and report RMSE\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "print(f\"Linear Regression RMSE: {rmse_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f82b86-b8c0-43c7-84ee-9a11be18f73c",
   "metadata": {},
   "source": [
    "### Baseline Model 2: Decision Tree Regressor\n",
    "We will use the specified max_depth=6 to prevent the tree from overfitting. We also set random_state for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "402bf3e2-2846-4ab7-afe7-3211d21e7073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (Depth=6) RMSE: 110.8498\n"
     ]
    }
   ],
   "source": [
    "# 1. Initialize and train the model\n",
    "dt_model = DecisionTreeRegressor(max_depth=6, random_state=RANDOM_STATE)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Make predictions on the test set\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# 3. Calculate and report RMSE\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "print(f\"Decision Tree (Depth=6) RMSE: {rmse_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d700a-c21e-4bf9-b4d6-938d2fdc279b",
   "metadata": {},
   "source": [
    "### 5. Part A Conclusion: Establishing the Baseline\n",
    "\n",
    "We evaluated our two single models on the test set to establish a baseline performance metric. The Root Mean Squared Error (RMSE) for each model is as follows:\n",
    "\n",
    "* **Linear Regression RMSE:** 164.41\n",
    "* **Decision Tree (max\\_depth=6) RMSE:** 110.85\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "The **Decision Tree Regressor** (RMSE: 110.85) performed significantly better than the Linear Regression model (RMSE: 164.41). This suggests the relationships in the data (like time of day and weather) are highly non-linear, which the tree model can capture more effectively.\n",
    "\n",
    "Therefore, the **baseline performance metric** for this assignment is set at **110.85 RMSE**. The goal for our ensemble models is to improve upon this score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9dbc2e-3852-4a5a-b3d2-7402347f0b9c",
   "metadata": {},
   "source": [
    "# Part B: Ensemble Techniques for Bias and Variance Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c328bf-88f8-4a7e-8f90-e76c50d78362",
   "metadata": {},
   "source": [
    "## B.1 Bagging (Variance Reduction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f6f20ae-be39-4492-87e1-b7425f0f14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45b7b105-139f-49b3-9315-1c05c4ad473d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 3: Bagging Regressor ---\n",
      "Training simple Bagging Regressor (100 trees, max_depth=6)...\n",
      "Bagging Regressor (D=6) RMSE: 101.8245\n",
      "Baseline to Beat: 110.8500\n"
     ]
    }
   ],
   "source": [
    "### --- Model 3: Bagging Regressor  ---\n",
    "print(\"\\n--- Running Model 3: Bagging Regressor ---\")\n",
    "\n",
    "# Define the baseline RMSE we are trying to beat (from Part A)\n",
    "baseline_rmse = 110.85 \n",
    "\n",
    "\n",
    "bagging_model_simple = BaggingRegressor(\n",
    "    estimator=dt_model,   # using the base estimator (our baseline model)   \n",
    "    n_estimators=100,       \n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1               \n",
    ")\n",
    "\n",
    "print(\"Training simple Bagging Regressor (100 trees, max_depth=6)...\")\n",
    "bagging_model_simple.fit(X_train_scaled, y_train) \n",
    "\n",
    "y_pred_bagging_simple = bagging_model_simple.predict(X_test_scaled)\n",
    "rmse_bagging_simple = np.sqrt(mean_squared_error(y_test, y_pred_bagging_simple)) \n",
    "\n",
    "print(f\"Bagging Regressor (D=6) RMSE: {rmse_bagging_simple:.4f}\")\n",
    "print(f\"Baseline to Beat: {baseline_rmse:.4f}\") # This will work now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de32f18-8d5d-4258-8603-2ec664522938",
   "metadata": {},
   "source": [
    "Let us also try to a RamdomForest with tuned hyperparameters to see if it does any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b205a08-9144-46e8-9a6e-05f2fc74b79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 4: Tuned Random Forest ---\n",
      "\n",
      "Starting GridSearchCV for RandomForestRegressor...\n",
      "Fitting 3 folds for each of 16 candidates, totalling 48 fits\n",
      "\n",
      "--- RF GridSearch Complete ---\n",
      "Best parameters found: {'max_depth': 20, 'max_features': 1.0, 'min_samples_leaf': 1, 'n_estimators': 100}\n",
      "\n",
      "Best Tuned RandomForest RMSE: 87.0095\n",
      "Baseline to Beat: 110.8500\n"
     ]
    }
   ],
   "source": [
    "### --- Model 4: Tuned Random Forest (Expert Step) ---\n",
    "print(\"\\n--- Running Model 4: Tuned Random Forest ---\")\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],         # Number of trees\n",
    "    'max_depth': [15, 20],              # Deeper trees (low bias)\n",
    "    'max_features': ['sqrt', 1.0],      # 'sqrt' is RF, '1.0' is standard Bagging\n",
    "    'min_samples_leaf': [1, 2]        # Regularization\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=2,  # Set to 2 to see progress\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GridSearchCV for RandomForestRegressor...\")\n",
    "rf_grid_search.fit(X_train_scaled, y_train) # Fit on raw y_train\n",
    "\n",
    "print(\"\\n--- RF GridSearch Complete ---\")\n",
    "print(f\"Best parameters found: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# Evaluate the best model found by the grid search\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "y_pred_rf_best = best_rf_model.predict(X_test_scaled)\n",
    "rmse_rf_best = np.sqrt(mean_squared_error(y_test, y_pred_rf_best)) # Eval on raw y_test\n",
    "\n",
    "print(f\"\\nBest Tuned RandomForest RMSE: {rmse_rf_best:.4f}\")\n",
    "print(f\"Baseline to Beat: {baseline_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7f1ae-641c-42a4-b33c-d73394bd1697",
   "metadata": {},
   "source": [
    "## Discussion: Bagging & Variance Reduction\n",
    "\n",
    "**1. RMSE achieved**\n",
    "\n",
    "* **Single Decision Tree (Baseline) RMSE:** 110.85\n",
    "* **Bagging Regressor (100 Trees, `max_depth=6`) RMSE:** 101.82\n",
    "* **Tuned Random Forest (100 Trees, `max_depth=20`) RMSE:** 87.01\n",
    "\n",
    "**2. Discussion of Effectiveness**\n",
    "\n",
    "**Yes, the bagging technique was highly effective at reducing variance and improving model performance.**\n",
    "\n",
    "The evidence for this is clear and twofold:\n",
    "\n",
    "* **Initial Improvement:** Our baseline **single Decision Tree** (RMSE: 110.85) is a high-variance model, meaning its predictions are unstable and highly sensitive to the specific training data. By implementing a simple `BaggingRegressor`—which averages 100 of these same shallow trees—we achieved an immediate **RMSE reduction to 101.82**. This 8.1% improvement confirms our hypothesis: the averaging process \"smoothed out\" the individual trees' errors and reduced the ensemble's overall variance.\n",
    "\n",
    "* **Unlocking Full Potential:** The true power of bagging is in controlling models with *low bias* and *high variance*. Our initial `max_depth=6` trees were \"weak learners\" with high bias. Our tuned **`RandomForestRegressor`** (which is an advanced form of bagging) used deep, complex `max_depth=20` trees. A single one of these deep trees would have overfit the data and performed terribly. However, the ensemble-averaging of bagging effectively contained this high variance, slashing the error to **87.01 (a 21.5% improvement)**.\n",
    "\n",
    "**Conclusion:** Bagging successfully reduced the variance of our baseline model. Furthermore, this technique proved to be the key that unlocked the use of much deeper, more complex trees, leading to our best-performing model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb2064-f999-4513-a0ef-a12e920113f9",
   "metadata": {},
   "source": [
    "## B.2 Boosting (Bias Reduction):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dc40e3d-78b9-4894-b376-b17495ed71d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 5: Gradient Boosting Regressor ---\n",
      "Training Gradient Boosting Regressor (100 trees, max_depth=3)...\n",
      "\n",
      "Gradient Boosting RMSE: 111.1437\n",
      "Baseline to Beat: 110.8500\n",
      "Bagging/RF Best to Beat: 87.0095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Define our new \"best score\" to beat\n",
    "baseline_rmse = 110.85\n",
    "rf_best_rmse = 87.0095  # Our new best score from Part B\n",
    "\n",
    "print(f\"\\n--- Running Model 5: Gradient Boosting Regressor ---\")\n",
    "\n",
    "# 1. Initialize the model\n",
    "# max_depth=3, learning_rate=0.1, n_estimators=100 are common defaults\n",
    "gbr_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 2. Train the model\n",
    "print(\"Training Gradient Boosting Regressor (100 trees, max_depth=3)...\")\n",
    "gbr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Evaluate\n",
    "y_pred_gbr = gbr_model.predict(X_test_scaled)\n",
    "rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))\n",
    "\n",
    "print(f\"\\nGradient Boosting RMSE: {rmse_gbr:.4f}\")\n",
    "print(f\"Baseline to Beat: {baseline_rmse:.4f}\")\n",
    "print(f\"Bagging/RF Best to Beat: {rf_best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1533d9-5fec-42f6-bdc1-fa239cde082f",
   "metadata": {},
   "source": [
    "Our default Gradient Boosting model performed poorly (RMSE: 111.14), even worse than our baseline. This is a classic sign of **severe underfitting**. The model's parameters (100 trees, 3-level depth) were too simple to capture the data's complexity.\n",
    "\n",
    "To fix this and unleash the model's true power, we must perform **hyperparameter tuning**. We will now use `GridSearchCV` to test a more robust set of parameters:\n",
    "\n",
    "* **`n_estimators`:** More trees to allow for more correction steps.\n",
    "* **`max_depth`:** Deeper trees to reduce bias and capture complex interactions.\n",
    "* **`learning_rate`:** A different learning pace to find a more precise result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e442b86c-bcc6-4f86-9f69-d50e514282c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Model 6: Tuned Gradient Boosting Regressor ---\n",
      "\n",
      "Starting GridSearchCV for GradientBoostingRegressor...\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "\n",
      "--- GBR GridSearch Complete ---\n",
      "Best parameters found: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "\n",
      "Best Tuned Gradient Boosting RMSE: 80.0026\n",
      "Random Forest Best to Beat: 87.0095\n"
     ]
    }
   ],
   "source": [
    "### --- Model 6: Tuned Gradient Boosting Regressor ---\n",
    "print(\"\\n--- Running Model 6: Tuned Gradient Boosting Regressor ---\")\n",
    "\n",
    "# 1. Define the parameter grid for GridSearchCV\n",
    "gbr_param_grid = {\n",
    "    'n_estimators': [100, 200],         # Number of trees\n",
    "    'learning_rate': [0.1, 0.05],       # How fast the model learns\n",
    "    'max_depth': [3, 5]                 # Deeper trees\n",
    "}\n",
    "\n",
    "# 2. Initialize the base estimator\n",
    "gbr = GradientBoostingRegressor(random_state=RANDOM_STATE)\n",
    "\n",
    "# 3. Initialize GridSearchCV\n",
    "gbr_grid_search = GridSearchCV(\n",
    "    estimator=gbr,\n",
    "    param_grid=gbr_param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=2,  # Set to 2 to see progress\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\nStarting GridSearchCV for GradientBoostingRegressor...\")\n",
    "gbr_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\n--- GBR GridSearch Complete ---\")\n",
    "print(f\"Best parameters found: {gbr_grid_search.best_params_}\")\n",
    "\n",
    "# 4. Evaluate the best model\n",
    "best_gbr_model = gbr_grid_search.best_estimator_\n",
    "y_pred_gbr_best = best_gbr_model.predict(X_test_scaled)\n",
    "rmse_gbr_best = np.sqrt(mean_squared_error(y_test, y_pred_gbr_best))\n",
    "\n",
    "print(f\"\\nBest Tuned Gradient Boosting RMSE: {rmse_gbr_best:.4f}\")\n",
    "print(f\"Random Forest Best to Beat: {rf_best_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23522a7a-3c42-4f52-ac54-b30d399d5bfd",
   "metadata": {},
   "source": [
    "## Discussion: Boosting & Bias Reduction\n",
    "\n",
    "**1. RMSE Calculation**\n",
    "\n",
    "* **Single Decision Tree (Baseline) RMSE:** 110.85\n",
    "* **Best Bagging Ensemble (Tuned RF) RMSE:** 87.01\n",
    "* **Best Boosting Ensemble (Tuned GBR) RMSE:** 80.00\n",
    "\n",
    "**2. Discussion of Effectiveness**\n",
    "\n",
    "**Yes, the boosting technique achieved a significantly better result than both the single model and the bagging ensemble.**\n",
    "\n",
    "Our `GradientBoostingRegressor`, once tuned, produced the **lowest RMSE (80.00)** of all models tested. This result strongly supports our hypothesis that boosting effectively reduces bias.\n",
    "\n",
    "Here's the story the data tells:\n",
    "1.  **High Initial Bias:** Our single tree (110.85 RMSE) and the default GBR (111.14 RMSE) both suffered from high bias (underfitting).\n",
    "2.  **Bagging's Approach:** Our `RandomForest` (a bagging method) reduced this error to 87.01 by averaging many deep, *low-bias* trees. It effectively reduced *variance*.\n",
    "3.  **Boosting's (Superior) Approach:** The `GradientBoostingRegressor` took a different, more direct route. It sequentially built models, with each new model laser-focused on correcting the errors (the *bias*) of the previous one. By tuning for deeper trees (`max_depth=5`) and more correction steps (`n_estimators=200`), we gave the model the power and time it needed to systematically \"boost\" itself and drive down this initial bias, ultimately achieving the lowest error score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e35fd1-b172-4743-b250-424f88000faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
